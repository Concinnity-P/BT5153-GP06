{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import stanza\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>languages</th>\n",
       "      <th>num of languages</th>\n",
       "      <th>summary</th>\n",
       "      <th>education</th>\n",
       "      <th>num of education</th>\n",
       "      <th>experiences</th>\n",
       "      <th>num of experiences</th>\n",
       "      <th>skill_k50_accounting / financial reporting / auditing</th>\n",
       "      <th>skill_k50_analysis / financial analysis / finance</th>\n",
       "      <th>...</th>\n",
       "      <th>language_taiwanese</th>\n",
       "      <th>language_tamil</th>\n",
       "      <th>language_telugu</th>\n",
       "      <th>language_thai</th>\n",
       "      <th>language_tigrinya</th>\n",
       "      <th>language_turkish</th>\n",
       "      <th>language_ukrainian</th>\n",
       "      <th>language_urdu</th>\n",
       "      <th>language_vietnamese</th>\n",
       "      <th>combined_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1033137</td>\n",
       "      <td>['english', 'spanish']</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Director of biopharma equity research and publ...</td>\n",
       "      <td>NYU Stern School of Business\\nMaster of Busine...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Evercore : Director - Biotechnology &amp; Pharmace...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>summary: Director of biopharma equity research...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1098586</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>blank</td>\n",
       "      <td>Fordham Gabelli School of Business\\nMaster of ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>- : Research Analyst\\nNone.-.\\nNo description\\...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>summary: blank;education: Fordham Gabelli Scho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1115736</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I am the CEO of JLab. We are the fastest growi...</td>\n",
       "      <td>University of Oklahoma\\nNA : NA\\n1995-1999\\n\\n...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>JLab Audio : President\\n2011.11-.\\nNo descript...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>summary: I am the CEO of JLab. We are the fast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1341457</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I offer ~20 years’ experience on the evolution...</td>\n",
       "      <td>University of Wisconsin-Madison\\nBBA : Finance...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>SLR Capital Partners : Partner\\n2023.1-.\\nNo d...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>summary: I offer ~20 years’ experience on the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1501168</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>We specialize in providing long-term financial...</td>\n",
       "      <td>University of Arkansas\\nBachelor of Science (B...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Boston Mountain Money Management, Inc. : Princ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>summary: We specialize in providing long-term ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id               languages  num of languages  \\\n",
       "0  1033137  ['english', 'spanish']               2.0   \n",
       "1  1098586                      []               0.0   \n",
       "2  1115736                      []               0.0   \n",
       "3  1341457                      []               0.0   \n",
       "4  1501168                      []               0.0   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Director of biopharma equity research and publ...   \n",
       "1                                              blank   \n",
       "2  I am the CEO of JLab. We are the fastest growi...   \n",
       "3  I offer ~20 years’ experience on the evolution...   \n",
       "4  We specialize in providing long-term financial...   \n",
       "\n",
       "                                           education  num of education  \\\n",
       "0  NYU Stern School of Business\\nMaster of Busine...               3.0   \n",
       "1  Fordham Gabelli School of Business\\nMaster of ...               1.0   \n",
       "2  University of Oklahoma\\nNA : NA\\n1995-1999\\n\\n...               2.0   \n",
       "3  University of Wisconsin-Madison\\nBBA : Finance...               3.0   \n",
       "4  University of Arkansas\\nBachelor of Science (B...               1.0   \n",
       "\n",
       "                                         experiences  num of experiences  \\\n",
       "0  Evercore : Director - Biotechnology & Pharmace...                 6.0   \n",
       "1  - : Research Analyst\\nNone.-.\\nNo description\\...                 3.0   \n",
       "2  JLab Audio : President\\n2011.11-.\\nNo descript...                 7.0   \n",
       "3  SLR Capital Partners : Partner\\n2023.1-.\\nNo d...                 6.0   \n",
       "4  Boston Mountain Money Management, Inc. : Princ...                 4.0   \n",
       "\n",
       "   skill_k50_accounting / financial reporting / auditing  \\\n",
       "0                                                  1       \n",
       "1                                                  0       \n",
       "2                                                  0       \n",
       "3                                                  0       \n",
       "4                                                  1       \n",
       "\n",
       "   skill_k50_analysis / financial analysis / finance  ...  language_taiwanese  \\\n",
       "0                                                  1  ...                   0   \n",
       "1                                                  1  ...                   0   \n",
       "2                                                  0  ...                   0   \n",
       "3                                                  1  ...                   0   \n",
       "4                                                  1  ...                   0   \n",
       "\n",
       "   language_tamil  language_telugu  language_thai  language_tigrinya  \\\n",
       "0               0                0              0                  0   \n",
       "1               0                0              0                  0   \n",
       "2               0                0              0                  0   \n",
       "3               0                0              0                  0   \n",
       "4               0                0              0                  0   \n",
       "\n",
       "   language_turkish  language_ukrainian  language_urdu  language_vietnamese  \\\n",
       "0                 0                   0              0                    0   \n",
       "1                 0                   0              0                    0   \n",
       "2                 0                   0              0                    0   \n",
       "3                 0                   0              0                    0   \n",
       "4                 0                   0              0                    0   \n",
       "\n",
       "                                       combined_text  \n",
       "0  summary: Director of biopharma equity research...  \n",
       "1  summary: blank;education: Fordham Gabelli Scho...  \n",
       "2  summary: I am the CEO of JLab. We are the fast...  \n",
       "3  summary: I offer ~20 years’ experience on the ...  \n",
       "4  summary: We specialize in providing long-term ...  \n",
       "\n",
       "[5 rows x 118 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('mix_combined.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Preprocess combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Tina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51927de4b8c4907aae44c5b689c5e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 14:34:46 INFO: Downloaded file to C:\\Users\\Tina\\stanza_resources\\resources.json\n",
      "2024-04-28 14:34:46 INFO: Downloading these customized packages for language: en (English)...\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| lemma     | combined_nocharlm |\n",
      "=================================\n",
      "\n",
      "2024-04-28 14:34:47 INFO: File exists: C:\\Users\\Tina\\stanza_resources\\en\\lemma\\combined_nocharlm.pt\n",
      "2024-04-28 14:34:47 INFO: Finished downloading models and saved to C:\\Users\\Tina\\stanza_resources\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stanza.download('en', processors='lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 14:34:47 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e5b915e175f436a83ef079faf8d3c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 14:34:49 INFO: Downloaded file to C:\\Users\\Tina\\stanza_resources\\resources.json\n",
      "2024-04-28 14:34:49 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-04-28 14:34:49 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| lemma     | combined_nocharlm |\n",
      "=================================\n",
      "\n",
      "2024-04-28 14:34:49 INFO: Using device: cpu\n",
      "2024-04-28 14:34:49 INFO: Loading: tokenize\n",
      "2024-04-28 14:34:49 INFO: Loading: mwt\n",
      "2024-04-28 14:34:49 INFO: Loading: lemma\n",
      "2024-04-28 14:34:49 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Didn't do stemming since it would cost overstemming problem.\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()    \n",
    "    # Remove URLs, mentions, and hashtags\n",
    "    text = re.sub(r'http\\S+|@\\S+|#\\S+', '', text)\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Perform lemmatization\n",
    "    doc = nlp(text)\n",
    "    lemmatized_tokens = [word.lemma for sent in doc.sentences for word in sent.words]\n",
    "    #lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    # Join the preprocessed tokens back into a string\n",
    "    preprocessed_text = ' '.join(lemmatized_tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_combined_text']=df['combined_text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'summary director of biopharma equity research and publishing analyst on the rank sellside team on wall street biotechpharma industry veteran with prove success at analyze clinical trial assess drug market potential and at cultivate relationship with physicianthought leader in order to drive strategic business objective extremely knowledgeable about the fda drug approval process and other key driver of the biopharma industry education nyu stern school of business master of business administration mba finance management fev fundao getulio vargas to do business in brazil rutger university pharmd pharmacy nonenoneexperiencence evercore director biotechnology pharmaceuticalsmajor equity research no description evercore vice president biotechnology pharmaceuticalsmajor equity research cover biotech major pharma and specialty pharma company under head analyst umer raffat and previously under dr mark schoenebaum ii rank in large cap biotech in large cap pharma in specialty pharma ii rank in large cap biotech in large cap pharma in specialty pharma ii rank in large cap biotech in large cap pharma in specialty pharma in smid cap biotech ii rank in large cap pharma large cap biotech and smid cap biotech evercore equity research associate biotechnology pharmaceuticalsmajor cover biotechnology company as well as major pharmaceutical company under headanalyst dr mark schoenebaum and umer raffat kerburn rise investment banking analyst select transaction experience potential capital raise consideration for developmental stage biotechnology company conduct extensive due diligence and use bottomup riskadjusted analysis to project global revenue for two phase iii product across indication presentation send to client cfo and business development team for consideration capital raise consideration for a pain therapeutics specialty pharmaceutical company and a biosimilar therapeutics company for each project lead two team of student intern to complete the follow deliverable full operate model with revenue build up comparable company analysis and dcf valuation sensitivity analysis table on capital structure and shareholder dilution strategic consideration pitch book mm lbo analysis of recreational vehicle company build scenariobased financial model to perform dcf and comparable company analysis incorporate lbo model with tranch of debt bank loan subordinated note pik loan and revolving line of credit to generate y irr construct irr sensitivity analysis base on exit year purchase premium debtequity ratio and exit multiple strategic alternative for aesthetic medical device company construct fullscale operate model on aesthetic medical device company author pitch book present various strategic consideration to company management strategic alternative for distance learning education company conduct thorough due diligence of the online education industry and build financial model value entity biogen sr medical science liaison multiple sclerosis and emerge pipeline therapy provide medical information and research support to gastroenterolist for tysabris use in crohn disease consult think leader physician on scientific datum regard biogen phase early stage and midstage pipeline product manage msl field interaction with nephrology physician researcher regard a phase ii lupus nephritis study interfaced with global clinical trial management to update they on progress conduct market and disease state due diligence to aid joint venture investment effort for phase iii study plan of neurology orphan drug attend investor presentation and relayed insight glean from key medical datum and wall street analyst question back to msl team cultivated investigatorinitiated avonex study proposal analyze and present their strategic merit internally to secure financing approval significantly add to the marketability of a billion revenue ms drug routinely attend medical congress meeting to gather key information on ms product by have discussion with physician during poster presentation support prelaunch effort of tecfidera which involve a key clinical presentation to the global medical affair department biogen medical science liaison multiple sclerosis no description'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'preprocessed_combined_text']\n",
    "#df.loc[0,'combined_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 8.58 GiB for an array with shape (3388, 339885) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m      2\u001b[0m bow_matrix \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_combined_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m bow_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mbow_matrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n\u001b[0;32m      5\u001b[0m whole_bow_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([bow_df,df],axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe number of columns in df is:\u001b[39m\u001b[38;5;124m'\u001b[39m, df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \\\n\u001b[0;32m      8\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe number of columns in bow_df is:\u001b[39m\u001b[38;5;124m'\u001b[39m, bow_df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\\\n\u001b[0;32m      9\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe number of columns in whole_bow_df is:\u001b[39m\u001b[38;5;124m'\u001b[39m, whole_bow_df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Tina\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_compressed.py:1051\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1050\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1051\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Tina\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_base.py:1298\u001b[0m, in \u001b[0;36mspmatrix._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 8.58 GiB for an array with shape (3388, 339885) and data type int64"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "bow_matrix = vectorizer.fit_transform(df['preprocessed_combined_text'])\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "whole_bow_df = pd.concat([bow_df,df],axis=1)\n",
    "\n",
    "print('The number of columns in df is:', df.shape[1], \\\n",
    "      'The number of columns in bow_df is:', bow_df.shape[1],\\\n",
    "      'The number of columns in whole_bow_df is:', whole_bow_df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_matrix = tfidf_transformer.fit_transform(df['preprocessed_combined_text'])\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "whole_tfidf_df = pd.concat([tfidf_df,df],axis=1)\n",
    "print('The number of columns in df is:', df.shape[1], \\\n",
    "      'The number of columns in tfidf_df is:', tfidf_df.shape[1],\\\n",
    "      'The number of columns in whole_tfidf_df is:', whole_tfidf_df.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Reduce dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_columns = whole_bow_df.columns[whole_bow_df.columns.str.startswith('skill_k50')]\n",
    "bow_df_x = whole_bow_df.drop(selected_columns, axis=1)\n",
    "# drop string columns in whole_bow_df\n",
    "bow_df_x=bow_df_x.drop(['combined_text','preprocessed_combined_text','languages','summary','education','experiences','user_id'],axis=1)\n",
    "bow_df_y = whole_bow_df.loc[:, selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = whole_tfidf_df.columns[whole_bow_df.columns.str.startswith('skill_k50')]\n",
    "tfidf_df_x = whole_tfidf_df.drop(selected_columns, axis=1)\n",
    "# drop string columns in whole_bow_df\n",
    "tfidf_df_x=tfidf_df_x.drop(['combined_text','preprocessed_combined_text','languages','summary','education','experiences','user_id'],axis=1)\n",
    "tfidf_df_y = whole_tfidf_df.loc[:, selected_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m new_column_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(trunc\u001b[38;5;241m.\u001b[39mn_components)]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Convert the transformed matrix to a DataFrame\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m bow_df_trunc \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(bow_array_trunc, columns\u001b[38;5;241m=\u001b[39m\u001b[43mnew_column_names\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m())\n\u001b[0;32m     12\u001b[0m bow_df_trunc \u001b[38;5;241m=\u001b[39m bow_df_trunc\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39mnew_column_names)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "trunc = TruncatedSVD(n_components=200)\n",
    "\n",
    "bow_array_x = bow_df_x.values\n",
    "# Fit and transform the bow_matrix\n",
    "bow_array_trunc = trunc.fit_transform(bow_array_x)\n",
    "\n",
    "new_column_names = [str(i + 1) for i in range(trunc.n_components)]\n",
    "# Convert the transformed matrix to a DataFrame\n",
    "bow_df_trunc_x = pd.DataFrame(bow_array_trunc, columns=new_column_names)\n",
    "bow_df_trunc_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the bow_matrix\n",
    "tfidf_array_x = tfidf_df_x.values\n",
    "\n",
    "tfidf_matrix_trunc = trunc.fit_transform(tfidf_array_x)\n",
    "\n",
    "new_column_names = [str(i + 1) for i in range(trunc.n_components)]\n",
    "\n",
    "# Convert the transformed matrix to a DataFrame\n",
    "tfidf_df_trunc_x = pd.DataFrame(tfidf_matrix_trunc, columns=new_column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of PCA\n",
    "pca = PCA(n_components=200)\n",
    "\n",
    "bow_array_x = bow_df_x.values\n",
    "# Fit and transform the bow_matrix\n",
    "bow_array_pca = pca.fit_transform(bow_array_x)\n",
    "\n",
    "# Create a list of column names as 1, 2, 3, ..., n_components\n",
    "new_column_names = [str(i + 1) for i in range(pca.n_components)]\n",
    "\n",
    "# Convert the transformed matrix to a DataFrame\n",
    "bow_df_pca_x = pd.DataFrame(bow_array_pca, columns=new_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of PCA\n",
    "\n",
    "tfidf_array_x = tfidf_df_x.values\n",
    "# Fit and transform the bow_matrix\n",
    "tfidf_array_pca = pca.fit_transform(tfidf_array_x)\n",
    "\n",
    "# Create a list of column names as 1, 2, 3, ..., n_components\n",
    "new_column_names = [str(i + 1) for i in range(pca.n_components)]\n",
    "\n",
    "# Convert the transformed matrix to a DataFrame\n",
    "tfidf_df_pca_x = pd.DataFrame(tfidf_array_pca, columns=new_column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Random Forest+bow+truncated svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.17\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.01      0.02        81\n",
      "           1       0.90      1.00      0.95       613\n",
      "           2       0.00      0.00      0.00         8\n",
      "           3       0.50      0.01      0.02       115\n",
      "           4       0.00      0.00      0.00        39\n",
      "           5       0.50      0.02      0.04        97\n",
      "           6       0.00      0.00      0.00        31\n",
      "           7       0.00      0.00      0.00        34\n",
      "           8       0.00      0.00      0.00        20\n",
      "           9       0.00      0.00      0.00       110\n",
      "          10       0.00      0.00      0.00        41\n",
      "          11       0.00      0.00      0.00        30\n",
      "          12       0.00      0.00      0.00        22\n",
      "          13       0.00      0.00      0.00         1\n",
      "          14       0.00      0.00      0.00         1\n",
      "          15       0.00      0.00      0.00        27\n",
      "          16       0.00      0.00      0.00        15\n",
      "          17       0.00      0.00      0.00        15\n",
      "          18       0.00      0.00      0.00        24\n",
      "          19       0.00      0.00      0.00        23\n",
      "          20       0.00      0.00      0.00        17\n",
      "          21       0.00      0.00      0.00        42\n",
      "          22       0.00      0.00      0.00        13\n",
      "          23       0.00      0.00      0.00        11\n",
      "          24       0.63      0.49      0.55       296\n",
      "          25       0.67      0.04      0.08       142\n",
      "          26       0.00      0.00      0.00        18\n",
      "          27       0.00      0.00      0.00        18\n",
      "          28       0.00      0.00      0.00        33\n",
      "          29       0.00      0.00      0.00        18\n",
      "          30       0.00      0.00      0.00         2\n",
      "          31       0.00      0.00      0.00        42\n",
      "          32       0.00      0.00      0.00        57\n",
      "          33       0.00      0.00      0.00         5\n",
      "          34       0.00      0.00      0.00        28\n",
      "          35       0.83      0.06      0.12        79\n",
      "          36       0.00      0.00      0.00         6\n",
      "          37       0.00      0.00      0.00        43\n",
      "          38       0.20      0.01      0.01       143\n",
      "          39       1.00      0.03      0.06        35\n",
      "          40       1.00      0.02      0.03        66\n",
      "          41       0.00      0.00      0.00        26\n",
      "          42       0.00      0.00      0.00        13\n",
      "          43       0.00      0.00      0.00        56\n",
      "          44       0.00      0.00      0.00        42\n",
      "          45       0.62      0.45      0.52       294\n",
      "          46       0.00      0.00      0.00        18\n",
      "          47       0.00      0.00      0.00        24\n",
      "          48       0.00      0.00      0.00        11\n",
      "          49       0.00      0.00      0.00        17\n",
      "\n",
      "   micro avg       0.79      0.31      0.44      2962\n",
      "   macro avg       0.14      0.04      0.05      2962\n",
      "weighted avg       0.45      0.31      0.31      2962\n",
      " samples avg       0.84      0.47      0.54      2962\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tina\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(bow_df_trunc_x, bow_df_y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "multi_target_RF = MultiOutputClassifier(model_RF, n_jobs=-1)\n",
    "\n",
    "multi_target_RF.fit(X_train, y_train)\n",
    "\n",
    "y_pred=multi_target_RF.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "print(classification_report(y_test, y_pred, target_names=y_test.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest+bow+PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(bow_df_pca_x, bow_df_y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "multi_target_RF = MultiOutputClassifier(model_RF, n_jobs=-1)\n",
    "\n",
    "multi_target_RF.fit(X_train, y_train)\n",
    "\n",
    "y_pred=multi_target_RF.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "print(classification_report(y_test, y_pred, target_names=y_test.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest+TF-IDF+truncated svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_df_trunc_x, tfidf_df_y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "multi_target_RF = MultiOutputClassifier(model_RF, n_jobs=-1)\n",
    "\n",
    "multi_target_RF.fit(X_train, y_train)\n",
    "\n",
    "y_pred=multi_target_RF.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "print(classification_report(y_test, y_pred, target_names=y_test.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random Forest+TF-IDF+PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_df_pca_x, tfidf_df_y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_RF = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "multi_target_RF = MultiOutputClassifier(model_RF, n_jobs=-1)\n",
    "\n",
    "multi_target_RF.fit(X_train, y_train)\n",
    "\n",
    "y_pred=multi_target_RF.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "print(classification_report(y_test, y_pred, target_names=y_test.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LightGBM+bow+truncated svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(bow_df_trunc_x, bow_df_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize LightGBM classifier and MultiOutputClassifier\n",
    "model_LGBM = lgb.LGBMClassifier(n_estimators=100, random_state=42, learning_rate=0.1)\n",
    "multi_target_LGBM = MultiOutputClassifier(model_LGBM, n_jobs=-1)\n",
    "\n",
    "# Fit model on training data\n",
    "multi_target_LGBM.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = multi_target_LGBM.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(classification_report(y_test, y_pred, target_names=y_test.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LightGBM+bow+PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(bow_df_pca_x, bow_df_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize LightGBM classifier and MultiOutputClassifier\n",
    "model_LGBM = lgb.LGBMClassifier(n_estimators=100, random_state=42, learning_rate=0.1)\n",
    "multi_target_LGBM = MultiOutputClassifier(model_LGBM, n_jobs=-1)\n",
    "\n",
    "# Fit model on training data\n",
    "multi_target_LGBM.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = multi_target_LGBM.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(classification_report(y_test, y_pred, target_names=y_test.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LightGBM+TF-IDF+truncated svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_df_trunc_x, tfidf_df_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize LightGBM classifier and MultiOutputClassifier\n",
    "model_LGBM = lgb.LGBMClassifier(n_estimators=100, random_state=42, learning_rate=0.1)\n",
    "multi_target_LGBM = MultiOutputClassifier(model_LGBM, n_jobs=-1)\n",
    "\n",
    "# Fit model on training data\n",
    "multi_target_LGBM.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = multi_target_LGBM.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(classification_report(y_test, y_pred, target_names=y_test.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. LightGBM+TF-IDF+PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_df_pca_x, tfidf_df_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize LightGBM classifier and MultiOutputClassifier\n",
    "model_LGBM = lgb.LGBMClassifier(n_estimators=100, random_state=42, learning_rate=0.1)\n",
    "multi_target_LGBM = MultiOutputClassifier(model_LGBM, n_jobs=-1)\n",
    "\n",
    "# Fit model on training data\n",
    "multi_target_LGBM.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = multi_target_LGBM.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(classification_report(y_test, y_pred, target_names=y_test.columns))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
